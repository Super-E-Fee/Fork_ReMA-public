trainer:
  nnodes: 1
  n_gpus_per_node: 4

workspace: /fs-computility/mabasic/wanziyu/workspace

data:
  path: ${workspace}/Code/ReMA-private/data/overall_math/test.parquet
  prompt_key: prompt
  n_samples: 1
  output_path: ${workspace}/Code/ReMA-private/output/q257bi/overall_math_test.parquet
  batch_size: 128

multi_agent:
  parameter_sharing: False
  mta_model: ${workspace}/checkpoints/ReMA/sft/single_turn_mamrp/mta_7b_full
  ra_model: ${workspace}/checkpoints/ReMA/sft/single_turn_mamrp/ra_7b_full

model:
  path: null
  external_lib: null
rollout:
  max_num_turns: 1
  name: vllm
  temperature: 0.0
  top_k: -1 # 0 for hf rollout, -1 for vllm rollout
  top_p: 1.0
  prompt_length: 14336
  response_length: 2048
  # for vllm rollout
  dtype: bfloat16 # should align with FSDP
  gpu_memory_utilization: 0.5
  ignore_eos: False
  enforce_eager: False
  enable_sleep_mode: False
  free_cache_engine: False
  load_format: dummy_dtensor
  tensor_model_parallel_size: 1
  max_num_batched_tokens: 16384
  max_model_len: null
  max_num_seqs: 1024
  log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu
  log_prob_micro_batch_size_per_gpu: 8
  # for fire vllm rollout
  use_fire_sampling: False # enable FIRE https://arxiv.org/abs/2410.21236
  # for hf rollout
  do_sample: True
  disable_log_stats: True
  enable_chunked_prefill: True
  n: 1
actor:
  strategy: fsdp  # This is for backward-compatibility
  ulysses_sequence_parallel_size: 1 # sp size
  fsdp_config:
    fsdp_size: -1